{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"State management for the index graph.\"\"\"\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Annotated, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, TypedDict, List, Dict, Union, Annotated\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "import operator\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    resume: str\n",
    "    job_description: str\n",
    "    job_title: str\n",
    "    company_name: str\n",
    "    history: List[Dict[str, str]] = field(default_factory=list)\n",
    "    generated_question: Optional[Union[AgentAction, AgentFinish, Dict[str, str]]] = None\n",
    "    human_answer: Optional[str] = None\n",
    "    answer_evaluation: Optional[Union[AgentAction, AgentFinish, Dict[str, str]]] = None\n",
    "    candidate_evaluation: Optional[Union[AgentAction, AgentFinish, Dict[str, str]]] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['company_name', 'history', 'job_description', 'job_title', 'resume'] metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'interviewer_personal_question', 'lc_hub_commit_hash': 'eb77f5e41c8a773b0b6235e77ae9b7957e7d56628a04c42fe5fb1d43f56a1e1e'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['company_name', 'history', 'job_description', 'job_title', 'resume'], template='You are a world-class interviewer conducting an interview for the position of {job_title} at {company_name}. You have access to the candidate\\'s {resume}, the {job_description}, and the {history} of previous questions and answers.\\nYour task is to generate ONE insightful personal question for the interview. Focus on the candidate\\'s background, experience, and resume details.\\nConsider the following when generating your question:\\n- Tailor the question to the specific job role and company.\\n- Ensure the question aligns with the job requirements and assesses relevant skills.\\n- Adjust the difficulty based on the candidate\\'s previous responses in the {history}.\\n- Aim to uncover the candidate\\'s motivations, experiences, and personal qualities.\\nGenerate your response in the following format:\\n{{\\n  \"topic\": \"personal\",\\n  \"question\": \"The full text of the interview question\",\\n  \"answer\": \"A suggested model answer or evaluation criteria for the question\",\\n  \"complexity\": \"A number from 1 to 5, where 1 is least complex and 5 is most complex\"\\n}}\\nExample:\\n{{\\n  \"topic\": \"personal\",\\n  \"question\": \"Looking at your career progression, what has been the driving force behind your transitions from one role to another, and how does this position at {company_name} align with your long-term career goals?\",\\n  \"answer\": \"A strong answer would demonstrate self-awareness, strategic career planning, and alignment between personal goals and the company\\'s mission. The candidate should articulate clear motivations for past career moves and show how this role fits into their professional development plan. Look for evidence of research into {company_name} and how it connects to their aspirations.\",\\n  \"complexity\": 4\\n}}\\nRemember, you are generating only ONE question per call. Make it count and ensure it provides valuable insights into the candidate\\'s personal and professional background.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['company_name', 'history', 'job_description', 'job_title', 'resume'], template='You are a world-class interviewer conducting an interview for the position of {job_title} at {company_name}. You have access to the candidate\\'s {resume}, the {job_description}, and the {history} of previous questions and answers.\\nYour task is to generate ONE insightful personal question for the interview. Focus on the candidate\\'s background, experience, and resume details.\\nConsider the following when generating your question:\\n- Tailor the question to the specific job role and company.\\n- Ensure the question aligns with the job requirements and assesses relevant skills.\\n- Adjust the difficulty based on the candidate\\'s previous responses in the {history}.\\n- Aim to uncover the candidate\\'s motivations, experiences, and personal qualities.\\nGenerate your response in the following format:\\n{{\\n  \"topic\": \"The topic category of the question\",\\n  \"question\": \"The full text of the interview question\",\\n  \"answer\": \"A suggested model answer or evaluation criteria for the question\",\\n  \"complexity\": \"A number from 1 to 5, where 1 is least complex and 5 is most complex\"\\n}}\\nExample:\\n{{\\n  \"topic\": \"Career Motivation\",\\n  \"question\": \"Looking at your career progression, what has been the driving force behind your transitions from one role to another, and how does this position at {company_name} align with your long-term career goals?\",\\n  \"answer\": \"A strong answer would demonstrate self-awareness, strategic career planning, and alignment between personal goals and the company\\'s mission. The candidate should articulate clear motivations for past career moves and show how this role fits into their professional development plan. Look for evidence of research into {company_name} and how it connects to their aspirations.\",\\n  \"complexity\": 4\\n}}\\nRemember, you are generating only ONE question per call. Make it count and ensure it provides valuable insights into the candidate\\'s personal and professional background.'))]\n",
      "Error in generate_questions: \n",
      "Input cannot be empty. Please provide a valid response.\n"
     ]
    },
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got AgentState(resume='Name: Rahul Krishnamoorthy\\nEmail: rahul.krish@example.com\\nPhone: +1-234-567-8901\\nLinkedIn: linkedin.com/in/rahulkrish\\nGitHub: github.com/rahulkrish\\n\\nProfessional Summary\\nDynamic AI Engineer and Full-Stack Developer with 5+ years of experience designing, developing, and deploying innovative software solutions. Proficient in backend development, API integration, and building data pipelines. Skilled in integrating GPT models and deploying CI/CD pipelines using AWS services.\\n\\nTechnical Skills\\nProgramming Languages: Python, JavaScript, Java\\nFrameworks & Tools: FastAPI, Flask, Vue.js, LangChain\\nCloud Technologies: AWS (EKS, S3, SES, MSK)\\nDatabases: MongoDB, PostgreSQL\\nOther Skills: Kafka, CI/CD pipelines, Generative AI Integration\\nExperience\\nAI Engineer | Oros Tech LLC (Remote)\\nJan 2022 – Present\\n\\nDeveloped over 50+ RESTful APIs using FastAPI and Flask.\\nBuilt streaming pipelines using Kafka for real-time data processing.\\nIntegrated GPT-4 with LangChain to create intelligent question-generation systems for recruitment.\\nDeployed applications on AWS infrastructure using Docker and Kubernetes.\\nService Engineer | 247.ai\\nJun 2019 – Dec 2021\\n\\nCollaborated with clients to troubleshoot software issues and query databases for diagnostics.\\nDocumented and resolved over 500 technical support tickets with a 95% satisfaction rate.\\nEducation\\nMaster of Science in Artificial Intelligence\\nUniversity of California, Berkeley | 2019\\n\\nBachelor of Technology in Computer Science\\nIndian Institute of Technology, Madras | 2017', job_description='Die XXXLutz-Unternehmensgruppe betreibt über 370 Einrichtungshäuser in 14 europäischen Ländern und beschäftigt mehr als 27.100 Mitarbeiter. Mit einem Jahresumsatz von 6 Milliarden Euro gehört XXXLutz zu den größten Möbelhändlern der Welt. Zudem werden bereits 24 Onlineshops in den Vertriebsschienen XXXLutz, Möbelix und Mömax betrieben. Die Zentrale des 1945 gegründeten Familienunternehmens befindet sich in Wels.\\n\\n\\nWir als XXXLdigital Team arbeiten gemeinsam an der Zukunft des Möbelhandels. Dabei arbeiten wir nicht nur in Teams, sondern leben sie auch – den Zusammenhalt, das Gefühl, das Lernen. Wir bringen unsere Ideen ein und gestalten proaktiv die Zukunft – und das auf allen digitalen Touchpoints. Wir sind begeistert und ständig auf der Suche nach neuen Lösungswegen. Dafür suchen wir Gleichgesinnte, die diesen Weg mit uns gehen wollen und bereit sind sich neuen Herausforderungen zu stellen.\\n\\nAufgaben:\\nAufbau und Entwicklung der serviceorientierten XXXLutz AI Plattform und des dazu gehörigen CI/CD Prozesses für unsere Services\\nEntwicklung, Implementierung und Wartung der skalierbaren Infrastruktur für die Integration, Speicherung, Verarbeitung und Analyse von Daten auf Basis von Kubernetes und Kubeflow\\nMitwirkung bei der Auswahlentscheidung der einzusetzenden Toolchain sowie deren Weiterentwicklung und Automatisierung mittels Scripting\\nSetup und Wartung von containerisierten Apps (Docker) und eingesetzten AI Tools sowie Implementieren von Security Konzepten für unsere AI Plattform\\nMitarbeit an AI Projekten', job_title='Data Scientist', company_name='Tech Corp', history=[{'question': 'What are your strengths?', 'answer': 'Problem-solving and teamwork'}], generated_question='Could not generate questions. Details: ', human_answer='kj', answer_evaluation=None, candidate_evaluation=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 355\u001b[0m\n\u001b[1;32m    343\u001b[0m graph\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexGraph\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m agent_state \u001b[38;5;241m=\u001b[39m AgentState(\n\u001b[1;32m    348\u001b[0m     resume\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn Doe\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms resume text here...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    349\u001b[0m     job_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob description for a data scientist role...\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m     history\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are your strengths?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem-solving and teamwork\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n\u001b[1;32m    353\u001b[0m )\n\u001b[0;32m--> 355\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresume\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mName: Rahul Krishnamoorthy\u001b[39;49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;43mEmail: rahul.krish@example.com\u001b[39;49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;43mPhone: +1-234-567-8901\u001b[39;49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;43mLinkedIn: linkedin.com/in/rahulkrish\u001b[39;49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;43mGitHub: github.com/rahulkrish\u001b[39;49m\n\u001b[1;32m    361\u001b[0m \n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;43mProfessional Summary\u001b[39;49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;43mDynamic AI Engineer and Full-Stack Developer with 5+ years of experience designing, developing, and deploying innovative software solutions. Proficient in backend development, API integration, and building data pipelines. Skilled in integrating GPT models and deploying CI/CD pipelines using AWS services.\u001b[39;49m\n\u001b[1;32m    364\u001b[0m \n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;43mTechnical Skills\u001b[39;49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;43mProgramming Languages: Python, JavaScript, Java\u001b[39;49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;43mFrameworks & Tools: FastAPI, Flask, Vue.js, LangChain\u001b[39;49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;43mCloud Technologies: AWS (EKS, S3, SES, MSK)\u001b[39;49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;43mDatabases: MongoDB, PostgreSQL\u001b[39;49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;43mOther Skills: Kafka, CI/CD pipelines, Generative AI Integration\u001b[39;49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;43mExperience\u001b[39;49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;43mAI Engineer | Oros Tech LLC (Remote)\u001b[39;49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;43mJan 2022 – Present\u001b[39;49m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;43mDeveloped over 50+ RESTful APIs using FastAPI and Flask.\u001b[39;49m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;43mBuilt streaming pipelines using Kafka for real-time data processing.\u001b[39;49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;43mIntegrated GPT-4 with LangChain to create intelligent question-generation systems for recruitment.\u001b[39;49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;43mDeployed applications on AWS infrastructure using Docker and Kubernetes.\u001b[39;49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;43mService Engineer | 247.ai\u001b[39;49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;43mJun 2019 – Dec 2021\u001b[39;49m\n\u001b[1;32m    381\u001b[0m \n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;43mCollaborated with clients to troubleshoot software issues and query databases for diagnostics.\u001b[39;49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;43mDocumented and resolved over 500 technical support tickets with a 95\u001b[39;49m\u001b[38;5;132;43;01m% s\u001b[39;49;00m\u001b[38;5;124;43matisfaction rate.\u001b[39;49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;43mEducation\u001b[39;49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;43mMaster of Science in Artificial Intelligence\u001b[39;49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;43mUniversity of California, Berkeley | 2019\u001b[39;49m\n\u001b[1;32m    387\u001b[0m \n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;43mBachelor of Technology in Computer Science\u001b[39;49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;43mIndian Institute of Technology, Madras | 2017\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjob_description\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mDie XXXLutz-Unternehmensgruppe betreibt über 370 Einrichtungshäuser in 14 europäischen Ländern und beschäftigt mehr als 27.100 Mitarbeiter. Mit einem Jahresumsatz von 6 Milliarden Euro gehört XXXLutz zu den größten Möbelhändlern der Welt. Zudem werden bereits 24 Onlineshops in den Vertriebsschienen XXXLutz, Möbelix und Mömax betrieben. Die Zentrale des 1945 gegründeten Familienunternehmens befindet sich in Wels.\u001b[39;49m\n\u001b[1;32m    391\u001b[0m \n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;43mWir als XXXLdigital Team arbeiten gemeinsam an der Zukunft des Möbelhandels. Dabei arbeiten wir nicht nur in Teams, sondern leben sie auch – den Zusammenhalt, das Gefühl, das Lernen. Wir bringen unsere Ideen ein und gestalten proaktiv die Zukunft – und das auf allen digitalen Touchpoints. Wir sind begeistert und ständig auf der Suche nach neuen Lösungswegen. Dafür suchen wir Gleichgesinnte, die diesen Weg mit uns gehen wollen und bereit sind sich neuen Herausforderungen zu stellen.\u001b[39;49m\n\u001b[1;32m    394\u001b[0m \n\u001b[1;32m    395\u001b[0m \u001b[38;5;124;43mAufgaben:\u001b[39;49m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;43mAufbau und Entwicklung der serviceorientierten XXXLutz AI Plattform und des dazu gehörigen CI/CD Prozesses für unsere Services\u001b[39;49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;43mEntwicklung, Implementierung und Wartung der skalierbaren Infrastruktur für die Integration, Speicherung, Verarbeitung und Analyse von Daten auf Basis von Kubernetes und Kubeflow\u001b[39;49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;43mMitwirkung bei der Auswahlentscheidung der einzusetzenden Toolchain sowie deren Weiterentwicklung und Automatisierung mittels Scripting\u001b[39;49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;43mSetup und Wartung von containerisierten Apps (Docker) und eingesetzten AI Tools sowie Implementieren von Security Konzepten für unsere AI Plattform\u001b[39;49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;43mMitarbeit an AI Projekten\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjob_title\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData Scientist\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompany_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTech Corp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat are your strengths?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProblem-solving and teamwork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m agent_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is your experience with Python?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 years\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1249\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1248\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1249\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:835\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    828\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    829\u001b[0m     futures,\n\u001b[1;32m    830\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    831\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    832\u001b[0m )\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m    838\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1338\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1336\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1341\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1342\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1343\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/pregel/retry.py:66\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     64\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/utils.py:77\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace:\n\u001b[0;32m---> 77\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         config \u001b[38;5;241m=\u001b[39m merge_configs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:1626\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1623\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1624\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1625\u001b[0m         Output,\n\u001b[0;32m-> 1626\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1634\u001b[0m     )\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1636\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/pregel/write.py:69\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH \u001b[38;5;28;01melse\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     68\u001b[0m     ]\n\u001b[0;32m---> 69\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrites\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     73\u001b[0m     values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     74\u001b[0m         (write\u001b[38;5;241m.\u001b[39mchannel, val)\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites)\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write\u001b[38;5;241m.\u001b[39mskip_none \u001b[38;5;129;01mor\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     ]\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_write(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(values))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/pregel/write.py:70\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH \u001b[38;5;28;01melse\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     68\u001b[0m     ]\n\u001b[1;32m     69\u001b[0m     values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 70\u001b[0m         val \u001b[38;5;28;01mif\u001b[39;00m write\u001b[38;5;241m.\u001b[39mmapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites)\n\u001b[1;32m     72\u001b[0m     ]\n\u001b[1;32m     73\u001b[0m     values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     74\u001b[0m         (write\u001b[38;5;241m.\u001b[39mchannel, val)\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m val, write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites)\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m write\u001b[38;5;241m.\u001b[39mskip_none \u001b[38;5;129;01mor\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     ]\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_write(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(values))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/utils.py:89\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     83\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, config)\n\u001b[1;32m     84\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     85\u001b[0m         {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config}\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m     88\u001b[0m     )\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langgraph/graph/state.py:280\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_state_key\u001b[0;34m(input, config, key)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SKIP_WRITE\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mget(key, SKIP_WRITE)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got AgentState(resume='Name: Rahul Krishnamoorthy\\nEmail: rahul.krish@example.com\\nPhone: +1-234-567-8901\\nLinkedIn: linkedin.com/in/rahulkrish\\nGitHub: github.com/rahulkrish\\n\\nProfessional Summary\\nDynamic AI Engineer and Full-Stack Developer with 5+ years of experience designing, developing, and deploying innovative software solutions. Proficient in backend development, API integration, and building data pipelines. Skilled in integrating GPT models and deploying CI/CD pipelines using AWS services.\\n\\nTechnical Skills\\nProgramming Languages: Python, JavaScript, Java\\nFrameworks & Tools: FastAPI, Flask, Vue.js, LangChain\\nCloud Technologies: AWS (EKS, S3, SES, MSK)\\nDatabases: MongoDB, PostgreSQL\\nOther Skills: Kafka, CI/CD pipelines, Generative AI Integration\\nExperience\\nAI Engineer | Oros Tech LLC (Remote)\\nJan 2022 – Present\\n\\nDeveloped over 50+ RESTful APIs using FastAPI and Flask.\\nBuilt streaming pipelines using Kafka for real-time data processing.\\nIntegrated GPT-4 with LangChain to create intelligent question-generation systems for recruitment.\\nDeployed applications on AWS infrastructure using Docker and Kubernetes.\\nService Engineer | 247.ai\\nJun 2019 – Dec 2021\\n\\nCollaborated with clients to troubleshoot software issues and query databases for diagnostics.\\nDocumented and resolved over 500 technical support tickets with a 95% satisfaction rate.\\nEducation\\nMaster of Science in Artificial Intelligence\\nUniversity of California, Berkeley | 2019\\n\\nBachelor of Technology in Computer Science\\nIndian Institute of Technology, Madras | 2017', job_description='Die XXXLutz-Unternehmensgruppe betreibt über 370 Einrichtungshäuser in 14 europäischen Ländern und beschäftigt mehr als 27.100 Mitarbeiter. Mit einem Jahresumsatz von 6 Milliarden Euro gehört XXXLutz zu den größten Möbelhändlern der Welt. Zudem werden bereits 24 Onlineshops in den Vertriebsschienen XXXLutz, Möbelix und Mömax betrieben. Die Zentrale des 1945 gegründeten Familienunternehmens befindet sich in Wels.\\n\\n\\nWir als XXXLdigital Team arbeiten gemeinsam an der Zukunft des Möbelhandels. Dabei arbeiten wir nicht nur in Teams, sondern leben sie auch – den Zusammenhalt, das Gefühl, das Lernen. Wir bringen unsere Ideen ein und gestalten proaktiv die Zukunft – und das auf allen digitalen Touchpoints. Wir sind begeistert und ständig auf der Suche nach neuen Lösungswegen. Dafür suchen wir Gleichgesinnte, die diesen Weg mit uns gehen wollen und bereit sind sich neuen Herausforderungen zu stellen.\\n\\nAufgaben:\\nAufbau und Entwicklung der serviceorientierten XXXLutz AI Plattform und des dazu gehörigen CI/CD Prozesses für unsere Services\\nEntwicklung, Implementierung und Wartung der skalierbaren Infrastruktur für die Integration, Speicherung, Verarbeitung und Analyse von Daten auf Basis von Kubernetes und Kubeflow\\nMitwirkung bei der Auswahlentscheidung der einzusetzenden Toolchain sowie deren Weiterentwicklung und Automatisierung mittels Scripting\\nSetup und Wartung von containerisierten Apps (Docker) und eingesetzten AI Tools sowie Implementieren von Security Konzepten für unsere AI Plattform\\nMitarbeit an AI Projekten', job_title='Data Scientist', company_name='Tech Corp', history=[{'question': 'What are your strengths?', 'answer': 'Problem-solving and teamwork'}], generated_question='Could not generate questions. Details: ', human_answer='kj', answer_evaluation=None, candidate_evaluation=None)"
     ]
    }
   ],
   "source": [
    "\"\"\"This \"graph\" simply exposes an endpoint for a user to upload docs to be indexed.\"\"\"\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import Literal, Optional, Union\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai.chat_models import AzureChatOpenAI, ChatOpenAI\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.tools import HumanInputRun\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.runnables.base import Runnable\n",
    "\n",
    "from langsmith import Client\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "class BaseQuestion(BaseModel):\n",
    "    topic: str = Field(description=\"Topic of the question that has been generated\")\n",
    "    question: str = Field(description=\"Interview question for the candidate\")\n",
    "    answer: str = Field(description=\"Suggested answer or evaluation criteria for the question\")\n",
    "    complexity: int = Field(description=\"A number from 1 to 5, where 1 is least complex and 5 is most complex\")\n",
    "\n",
    "class PersonalQuestion(BaseModel):\n",
    "    topic: str = Field(description=\"Topic of the question that has been generated\")\n",
    "    question: str = Field(description=\"The full text of the HR question\")\n",
    "    answer: str = Field(description=\"A suggested model answer or evaluation criteria for the question\")\n",
    "    complexity: int = Field(description=\"A number from 1 to 5, where 1 is least complex and 5 is most complex\")\n",
    "\n",
    "class HRQuestion(BaseModel):\n",
    "    topic: str = Field(description=\"Topic of the question that has been generated\")\n",
    "    question: str = Field(description=\"The full text of the HR question\")\n",
    "    answer: str = Field(description=\"A suggested model answer or evaluation criteria for the question\")\n",
    "    complexity: int = Field(description=\"A number from 1 to 5, where 1 is least complex and 5 is most complex\")\n",
    "\n",
    "class CriticalQuestion(BaseModel):\n",
    "    topic: str = Field(description=\"Topic of the question that has been generated\")\n",
    "    scenario: str = Field(description=\"A detailed description of the situation or problem\")\n",
    "    question: str = Field(description=\"The specific question or task for the candidate based on the scenario\")\n",
    "    key_considerations: List[str] = Field(description=\"A list of important factors the candidate should consider in their response\")\n",
    "    evaluation_criteria: str = Field(description=\"Key points or approaches the candidate should demonstrate in their answer\")\n",
    "    follow_up: Optional[str] = Field(description=\"An optional follow-up question to probe deeper into the candidate's thinking\")\n",
    "    complexity: int = Field(description=\"A number from 1 to 5, where 1 is least complex and 5 is most complex\")\n",
    "\n",
    "class TechnicalQuestion(BaseModel):\n",
    "    topic: str = Field(description=\"Topic of the question that has been generated\")\n",
    "    question_type: Literal[\"Theoretical\", \"Coding Challenge\", \"System Architecture\"] = Field(description=\"Type of technical question\")\n",
    "    question: str = Field(description=\"The full text of the technical question or coding challenge\")\n",
    "    instructions: str = Field(description=\"Clear, step-by-step instructions for the candidate\")\n",
    "    example_input: Optional[str] = Field(description=\"Sample input for the problem, if applicable\")\n",
    "    example_output: Optional[str] = Field(description=\"Expected output for the sample input, if applicable\")\n",
    "    answer_criteria: str = Field(description=\"Key points or approach the candidate should demonstrate in their answer or solution\")\n",
    "    complexity: int = Field(description=\"A number from 1 to 5, where 1 is least complex and 5 is most complex\")\n",
    "\n",
    "\n",
    "class EvaluateAnswers(BaseModel):\n",
    "    question: str = Field(description=\"Question for which the evaluation is made\")\n",
    "    answer: str = Field(description=\"Answer for which the evaluation is made(human entered answer)\")\n",
    "    evaluation_summary: str = Field(description=\"Evaluation summary for the given question and answer that has been generated\")\n",
    "    relevance: int = Field(description=\"Relevance score of the answer\")\n",
    "    correctness: int = Field(description=\"Correctness score of the answer\")\n",
    "    leadership: int = Field(description=\"Leadership score assessed from the answer\")\n",
    "    team_work: int = Field(description=\"Team work score assessed from the answer\")\n",
    "    technical_strength: int = Field(description=\"Technical strength score assessed from the answer\")\n",
    "    communication: int = Field(description=\"Communication score assessed from the answer\")\n",
    "\n",
    "class CategoryEvaluation(BaseModel):\n",
    "    score: int = Field(..., description=\"Score from 1 (Poor) to 5 (Excellent)\")\n",
    "    examples: List[str] = Field(..., description=\"Specific examples supporting the evaluation\")\n",
    "\n",
    "class CandidateEvaluation(BaseModel):\n",
    "    overall_assessment: str = Field(..., description=\"Concise summary of the candidate's suitability for the job role\")\n",
    "    \n",
    "    technical_proficiency: CategoryEvaluation = Field(\n",
    "        ..., description=\"Evaluation of technical knowledge, problem-solving skills, and technical challenge handling (Weight: 25%)\"\n",
    "    )\n",
    "    communication_skills: CategoryEvaluation = Field(\n",
    "        ..., description=\"Evaluation of communication clarity, effectiveness, and ability to explain complex concepts (Weight: 20%)\"\n",
    "    )\n",
    "    cultural_fit: CategoryEvaluation = Field(\n",
    "        ..., description=\"Assessment of alignment with company culture and potential contribution to the team (Weight: 15%)\"\n",
    "    )\n",
    "    leadership_teamwork: CategoryEvaluation = Field(\n",
    "        ..., description=\"Evaluation of leadership potential and ability to collaborate effectively (Weight: 15%)\"\n",
    "    )\n",
    "    adaptability_learning_agility: CategoryEvaluation = Field(\n",
    "        ..., description=\"Assessment of adaptability, learning agility, openness to feedback, and continuous improvement (Weight: 15%)\"\n",
    "    )\n",
    "    relevant_experience: CategoryEvaluation = Field(\n",
    "        ..., description=\"Evaluation of alignment between past experiences and job role requirements (Weight: 10%)\"\n",
    "    )\n",
    "    \n",
    "    strengths: List[str] = Field(..., description=\"3-5 key strengths of the candidate with specific examples\")\n",
    "    areas_for_improvement: List[str] = Field(..., description=\"2-3 areas where the candidate could improve or need support\")\n",
    "    \n",
    "    final_recommendation: str = Field(\n",
    "        ..., description=\"Final recommendation: Strongly Recommend Hire, Recommend Hire, Recommend Additional Interview, or Do Not Recommend\"\n",
    "    )\n",
    "    \n",
    "    additional_comments: Optional[str] = Field(None, description=\"Additional insights or observations relevant to the hiring decision\")\n",
    "    \n",
    "load_dotenv()\n",
    "\n",
    "client = Client()\n",
    "\n",
    "model = ChatOpenAI(temperature=0,\n",
    "                            model_kwargs={\"seed\": 42},\n",
    "                            streaming=True,\n",
    "                            api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    "                            )\n",
    "\n",
    "def personal_questions(state: AgentState) -> Dict[str, Union[PersonalQuestion, str]]:\n",
    "    \"\"\"Generate interview questions using an LLM.\"\"\"\n",
    "    try:\n",
    "        document_prompt = client.pull_prompt(\"interviewer_personal_question\")\n",
    "        print(document_prompt)\n",
    "        chain = (document_prompt | model.with_structured_output(PersonalQuestion))\n",
    "        response = chain.invoke({\n",
    "            \"resume\": state.resume,\n",
    "            \"job_description\": state.job_description,\n",
    "            \"job_title\": state.job_title,\n",
    "            \"company_name\": state.company_name,\n",
    "            \"history\": state.history\n",
    "        })\n",
    "        print(response)\n",
    "\n",
    "        if not all(hasattr(response, field) for field in [\"topic\", \"question\"]):\n",
    "            raise ValueError(\"Generated question is missing required fields\")\n",
    "\n",
    "        return {\"generated_question\": response}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_questions: {e}\")\n",
    "        return {\"generated_question\": f\"Could not generate questions. Details: {str(e)}\"}\n",
    "    \n",
    "def critical_questions(state: AgentState) -> Dict[str, Union[CriticalQuestion, str]]:\n",
    "    \"\"\"Generate interview questions using an LLM.\"\"\"\n",
    "    try:\n",
    "        question_count = len(state.history)\n",
    "\n",
    "        document_prompt = client.pull_prompt(\"interviewer_critical_questions\")\n",
    "        chain = (document_prompt | model.with_structured_output(CriticalQuestion))\n",
    "        response = chain.invoke({\n",
    "            \"resume\": state.resume,\n",
    "            \"job_description\": state.job_description,\n",
    "            \"job_title\": state.job_title,\n",
    "            \"company_name\": state.company_name,\n",
    "            \"history\": state.history,\n",
    "            \"question_count\": question_count\n",
    "        })\n",
    "\n",
    "        if not all(hasattr(response, field) for field in [\"topic\", \"question\"]):\n",
    "            raise ValueError(\"Generated question is missing required fields\")\n",
    "\n",
    "        return {\"generated_question\": response}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_questions: {e}\")\n",
    "        return {\"error\": f\"Could not generate questions. Details: {str(e)}\"}\n",
    "\n",
    "def technical_questions(state: AgentState) -> Dict[str, Union[TechnicalQuestion, str]]:\n",
    "    \"\"\"Generate interview questions using an LLM.\"\"\"\n",
    "    try:\n",
    "        question_count = len(state.history)\n",
    "\n",
    "        document_prompt = client.pull_prompt(\"interviewer_technical_questions\")\n",
    "        chain = (document_prompt | model.with_structured_output(TechnicalQuestion))\n",
    "        response = chain.invoke({\n",
    "            \"resume\": state.resume,\n",
    "            \"job_description\": state.job_description,\n",
    "            \"job_title\": state.job_title,\n",
    "            \"company_name\": state.company_name,\n",
    "            \"history\": state.history,\n",
    "            \"question_count\": question_count\n",
    "        })\n",
    "\n",
    "        if not all(hasattr(response, field) for field in [\"topic\", \"question\"]):\n",
    "            raise ValueError(\"Generated question is missing required fields\")\n",
    "\n",
    "        return {\"generated_question\": response}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_questions: {e}\")\n",
    "        return {\"error\": f\"Could not generate questions. Details: {str(e)}\"}\n",
    "    \n",
    "def hr_questions(state: AgentState) -> Dict[str, Union[HRQuestion, str]]:\n",
    "    \"\"\"Generate interview questions using an LLM.\"\"\"\n",
    "    try:\n",
    "        question_count = len(state.history)\n",
    "\n",
    "        document_prompt = client.pull_prompt(\"interviewer_hr_questions\")\n",
    "        chain = (document_prompt | model.with_structured_output(HRQuestion))\n",
    "        response = chain.invoke({\n",
    "            \"resume\": state.resume,\n",
    "            \"job_description\": state.job_description,\n",
    "            \"job_title\": state.job_title,\n",
    "            \"company_name\": state.company_name,\n",
    "            \"history\": state.history,\n",
    "            \"question_count\": question_count\n",
    "        })\n",
    "\n",
    "        if not all(hasattr(response, field) for field in [\"topic\", \"question\"]):\n",
    "            raise ValueError(\"Generated question is missing required fields\")\n",
    "\n",
    "        return {\"generated_question\": response}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_questions: {e}\")\n",
    "        return {\"error\": f\"Could not generate questions. Details: {str(e)}\"}\n",
    "\n",
    "def human_input(state: AgentState) -> AgentState:\n",
    "    \"\"\"Accepts a human response and updates the state.\"\"\"\n",
    "    max_retries = 3\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = input()\n",
    "            \n",
    "            if not response.strip():\n",
    "                print(\"Input cannot be empty. Please provide a valid response.\")\n",
    "                retries += 1\n",
    "                continue\n",
    "            \n",
    "            state.human_answer = response\n",
    "            return state\n",
    "        except EOFError:\n",
    "            print(\"EOFError: Input stream is closed or unavailable.\")\n",
    "            retry = input(\"Do you want to retry? (y/n): \").lower()\n",
    "            if retry != 'y':\n",
    "                raise ValueError(\"User chose to exit after encountering an EOFError.\")\n",
    "            retries += 1\n",
    "\n",
    "    raise ValueError(\"Maximum retries reached. Unable to get valid input.\")\n",
    "\n",
    "def reset_entries(state: AgentState) -> AgentState:\n",
    "    \"\"\"Resets specific entries in the state.\"\"\"\n",
    "    state.generated_question = None\n",
    "    state.answer_evaluation = None\n",
    "    state.human_answer = None\n",
    "    return state\n",
    "\n",
    "def evaluate_answers(state: AgentState) -> Dict[str, Union[EvaluateAnswers, str]]:\n",
    "    \"\"\"Generate evaluation for the human answer.\"\"\"\n",
    "    try:\n",
    "        document_prompt = client.pull_prompt(\"interviewer_answer_evaluator_prompt\")\n",
    "        chain = (document_prompt | model.with_structured_output(EvaluateAnswers))\n",
    "        evaluation_response = chain.invoke({\n",
    "            \"resume\": state.resume,\n",
    "            \"job_description\": state.job_description,\n",
    "            \"job_title\": state.job_title,\n",
    "            \"company_name\": state.company_name,\n",
    "            \"question\": state.generated_question.question if isinstance(state.generated_question) else str(state.generated_question),\n",
    "            \"answer\": state.human_answer,\n",
    "            \"history\": state.history\n",
    "        })\n",
    "        \n",
    "        state.history.append(evaluation_response.dict())\n",
    "        return {\"answer_evaluation\": evaluation_response}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluate_answers: {e}\")\n",
    "        return {\"answer_evaluation\": f\"Could not process question and answer. Details: {str(e)}\"}\n",
    "\n",
    "def count_questions(state: AgentState) -> str:\n",
    "    \"\"\"Determines whether to finish based on the number of questions asked.\"\"\"\n",
    "    if len(state.history) >= 20:\n",
    "        return \"evaluate_candidate\"\n",
    "    else:\n",
    "        return \"generate_questions\"\n",
    "    \n",
    "def evaluate_candidate(state: AgentState)-> Dict[str, Union[CandidateEvaluation, str]]:\n",
    "    \"\"\"Generate candidate evaluation using an LLM.\"\"\"\n",
    "    try:\n",
    "        document_prompt = client.pull_prompt(\"candidate_evaluation_prompt\")\n",
    "        chain = (document_prompt | model.with_structured_output(CandidateEvaluation))\n",
    "        response = chain.invoke({ \"job_description\": state.job_description,\n",
    "                           \"job_role\": state.job_title,\n",
    "                           \"company_name\": state.company_name,\n",
    "                           \"history\": state.history})\n",
    "        return {\"candidate_evaluation\": response}\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return {\"candidate_evaluation\": f\"Could not generate candidate_evaluation. Details: {str(e)}\"}\n",
    "    \n",
    "def questions_topic_decider(state: AgentState) -> str:\n",
    "    \"\"\"Determines which topic to choose based on the number of questions asked.\"\"\"\n",
    "    if len(state.history) <= 7:\n",
    "        return \"personal_questions\"\n",
    "    elif len(state.history) <= 20:\n",
    "        return \"technical_questions\"\n",
    "    elif len(state.history) <= 27:\n",
    "        return \"critical_questions\"\n",
    "    else:\n",
    "        return \"hr_questions\" \n",
    "    \n",
    "# Define the graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"technical_questions\", technical_questions)\n",
    "builder.add_node(\"hr_questions\", hr_questions)\n",
    "builder.add_node(\"critical_questions\", critical_questions)\n",
    "builder.add_node(\"personal_questions\", personal_questions)\n",
    "builder.add_node(\"human_input\", human_input)\n",
    "builder.add_node(\"evaluate_answers\", evaluate_answers)\n",
    "builder.add_node(\"reset_entries\", reset_entries)\n",
    "builder.add_node(\"evaluate_candidate\", evaluate_candidate)\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"personal_questions\")\n",
    "builder.add_edge(\"personal_questions\", \"human_input\")\n",
    "builder.add_edge(\"technical_questions\", \"human_input\")\n",
    "builder.add_edge(\"critical_questions\", \"human_input\")\n",
    "builder.add_edge(\"hr_questions\", \"human_input\")\n",
    "builder.add_edge(\"human_input\", \"evaluate_answers\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"reset_entries\",\n",
    "    questions_topic_decider,\n",
    "    {\n",
    "        \"personal_questions\": \"personal_questions\",\n",
    "        \"technical_questions\": \"technical_questions\",\n",
    "        \"critical_questions\": \"critical_questions\",\n",
    "        \"hr_questions\": \"hr_questions\"\n",
    "    }\n",
    ")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"evaluate_answers\", \n",
    "    count_questions,   \n",
    "    {                   \n",
    "        \"generate_questions\": \"reset_entries\", \n",
    "        \"evaluate_candidate\": \"evaluate_candidate\"\n",
    "    }\n",
    ")\n",
    "\n",
    "builder.add_edge(\"evaluate_candidate\", END)\n",
    "\n",
    "# Compile into a graph object that you can invoke and deploy.\n",
    "graph = builder.compile()\n",
    "graph.name = \"IndexGraph\"\n",
    "\n",
    "\n",
    "\n",
    "agent_state = AgentState(\n",
    "    resume=\"John Doe's resume text here...\",\n",
    "    job_description=\"Job description for a data scientist role...\",\n",
    "    job_title=\"Data Scientist\",\n",
    "    company_name=\"Tech Corp\",\n",
    "    history=[{\"question\": \"What are your strengths?\", \"answer\": \"Problem-solving and teamwork\"}]\n",
    ")\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"resume\": \"\"\"Name: Rahul Krishnamoorthy\n",
    "Email: rahul.krish@example.com\n",
    "Phone: +1-234-567-8901\n",
    "LinkedIn: linkedin.com/in/rahulkrish\n",
    "GitHub: github.com/rahulkrish\n",
    "\n",
    "Professional Summary\n",
    "Dynamic AI Engineer and Full-Stack Developer with 5+ years of experience designing, developing, and deploying innovative software solutions. Proficient in backend development, API integration, and building data pipelines. Skilled in integrating GPT models and deploying CI/CD pipelines using AWS services.\n",
    "\n",
    "Technical Skills\n",
    "Programming Languages: Python, JavaScript, Java\n",
    "Frameworks & Tools: FastAPI, Flask, Vue.js, LangChain\n",
    "Cloud Technologies: AWS (EKS, S3, SES, MSK)\n",
    "Databases: MongoDB, PostgreSQL\n",
    "Other Skills: Kafka, CI/CD pipelines, Generative AI Integration\n",
    "Experience\n",
    "AI Engineer | Oros Tech LLC (Remote)\n",
    "Jan 2022 – Present\n",
    "\n",
    "Developed over 50+ RESTful APIs using FastAPI and Flask.\n",
    "Built streaming pipelines using Kafka for real-time data processing.\n",
    "Integrated GPT-4 with LangChain to create intelligent question-generation systems for recruitment.\n",
    "Deployed applications on AWS infrastructure using Docker and Kubernetes.\n",
    "Service Engineer | 247.ai\n",
    "Jun 2019 – Dec 2021\n",
    "\n",
    "Collaborated with clients to troubleshoot software issues and query databases for diagnostics.\n",
    "Documented and resolved over 500 technical support tickets with a 95% satisfaction rate.\n",
    "Education\n",
    "Master of Science in Artificial Intelligence\n",
    "University of California, Berkeley | 2019\n",
    "\n",
    "Bachelor of Technology in Computer Science\n",
    "Indian Institute of Technology, Madras | 2017\"\"\",\n",
    "    \"job_description\": \"\"\"Die XXXLutz-Unternehmensgruppe betreibt über 370 Einrichtungshäuser in 14 europäischen Ländern und beschäftigt mehr als 27.100 Mitarbeiter. Mit einem Jahresumsatz von 6 Milliarden Euro gehört XXXLutz zu den größten Möbelhändlern der Welt. Zudem werden bereits 24 Onlineshops in den Vertriebsschienen XXXLutz, Möbelix und Mömax betrieben. Die Zentrale des 1945 gegründeten Familienunternehmens befindet sich in Wels.\n",
    "\n",
    "\n",
    "Wir als XXXLdigital Team arbeiten gemeinsam an der Zukunft des Möbelhandels. Dabei arbeiten wir nicht nur in Teams, sondern leben sie auch – den Zusammenhalt, das Gefühl, das Lernen. Wir bringen unsere Ideen ein und gestalten proaktiv die Zukunft – und das auf allen digitalen Touchpoints. Wir sind begeistert und ständig auf der Suche nach neuen Lösungswegen. Dafür suchen wir Gleichgesinnte, die diesen Weg mit uns gehen wollen und bereit sind sich neuen Herausforderungen zu stellen.\n",
    "\n",
    "Aufgaben:\n",
    "Aufbau und Entwicklung der serviceorientierten XXXLutz AI Plattform und des dazu gehörigen CI/CD Prozesses für unsere Services\n",
    "Entwicklung, Implementierung und Wartung der skalierbaren Infrastruktur für die Integration, Speicherung, Verarbeitung und Analyse von Daten auf Basis von Kubernetes und Kubeflow\n",
    "Mitwirkung bei der Auswahlentscheidung der einzusetzenden Toolchain sowie deren Weiterentwicklung und Automatisierung mittels Scripting\n",
    "Setup und Wartung von containerisierten Apps (Docker) und eingesetzten AI Tools sowie Implementieren von Security Konzepten für unsere AI Plattform\n",
    "Mitarbeit an AI Projekten\"\"\",\n",
    "    \"job_title\": \"Data Scientist\",\n",
    "    \"company_name\": \"Tech Corp\",\n",
    "    \"history\": [{\"question\": \"What are your strengths?\", \"answer\": \"Problem-solving and teamwork\"}]\n",
    "})\n",
    "agent_state[\"history\"].append({\"question\": \"What is your experience with Python?\", \"answer\": \"5 years\"})\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
